# Bootcamp of Deep Learning Models
## History of AI Summary
## Perceptron
## Deep Neural Networks (DNN)
### DNN for Classification
### DNN for Regression

## GPU Access
- GPU usage in Google Colab
- GPU usage in Kaggle

## Kaggle
- Code Version Control in Kaggle
- Data Version Control in Kaggle

## Keras
### Sequential and Functional APIs in Keras
### Initializers
### Callbacks in Keras
- Model Checkpoint
- Learning Rate Scheduler
- Early Stopper
- TensorBoard
- Custom History

## How to Train a DNN
1. Proprocess Data: Data Normalizationo
2. Weight Initialization
- Xavier/Glorot: sigmoid/tanh
- He: Relu
- Gaussian: samll $\sigma$
3. Choose activation function: Relu
4. Batch normalization: Especially for deep MLP later layers
5. Dropout: p dropout hyperparameter
6. Optimizer: Adam (Adaptive fast)
7. Hyperparameters
- Architectures: no. of layers, dropout rate, adam beta1&2
8. Loss function
- binary classification: log-loss
- k-class classification: multiclass log loss
- regression: squared loss
9. Monitor gradients: clipping
10. Monitor train-test curves
11. Avoid overfitting 


### Auto-Encoder
## Sequential Models
### Recurrent Neural Networks (RNN)
### Bidirectional RNN (Bi-RNN)
### Long Short Term Memory (LSTM)
### Gated Recurrent Unit(GRU)